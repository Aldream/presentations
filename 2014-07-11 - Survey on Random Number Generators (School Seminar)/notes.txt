# Introduction

## Hello World

Benjamin (Bill) Planche - French - Last year

## Subject

- Generation of random numbers = important operation for various computational tasks
- Not all applications have the same requirements about randomness
- Overview of the most common requirements, solutions, test tools

## Structure


# Randomness

## Introduction

- Lack of pattern, predictability or determinism in events.
- Is a structure really random, or are we  simply ignorant of a hidden underlying pattern?

Random Sequences:
- studied in Information Theory
- Simple Def: Sequence of independent random variables
- Formal/Mathematical Def -> Subject of many debates and studies during the last century.

## Definition by Von Mises

- 1st attempt at defining algorithmic randomness by Richard Edler von Mises (1883-1953)
- Base on Theory of Large Numbers (AVG of results from a large number of trials -> Expected Val.)
- An infinite sequence of symbols can be considered random if :
	- it possesses the frequency stability property (the appearance frequencies of the symbols aren't biased /goes to the same expected value)
	- any sub-sequence selected by a "proper method" isn't biased too.
		- Fundamental in this definition.
		- Ex: 10101010 is not biased, but we obtain the biased sub-sequence 0000 by selecting only the even positions.
		
- But unsatisfying :
	- How to mathematize what the "proper method of selection" was.
	- Demonstration by Jean Ville in 1939, proving that such a definition only yields an empty set.
	
## Definition by Martin-Löf

- Swedish statistician Per Martin-Löf. Def in 1966
- Random Seq:
	- has no "exceptional and effectively verifiable" property
	- i.e. has no properties we can verify by a recursive algorithm
- Based on the measure theory
- Most satisfactory frequency / measure-theoretic approach to randomness


## Definition by Levin/Chaitin

### Complexity of Kolmogorov

- Ray Solomonoff (1926-2009) and Andre Nikolaevitch Kolmogorov (1903-1987), in the 60's
- Important measure for Information Theory: complexity of Kolmogorov
- Def: the length of the shortest program (independently of the machine running it) able to generate the evaluated sequence.

### Complixity / Compressibility Def of Randomness

- Leonid Anatolievich Levin (1948) and Gregory Chaitin (1947), in 1975
- Def: random finite string = string which requires a program at least as long as itself to be computed
- Another way to express it is: "a random sequence must have an incomprehensible informational content"
	-> Impossible to make any sense of it, and thus to use a shorter sequence or program to describe it.
- Approach is considered as a complexity / compressibility one.

## Definition by Schnorr

- Claus-Peter Schnorr [1943), in 1971
- Predictability approach
- "random sequence must not be predictable. No effective strategy should lead to an infinite gain if we bet on the symbols of the sequence"

## Pseudorandomness
- Numeric sequence = statistically random if it contains no recognizable patterns or regularities.
- Less strict than prev. defs &  doesn't imply objective unpredictability -> leaves room to the concept of pseudo-randomness.
- Pseudo-random sequence exhibits statistical randomness while being generated by an entirely deterministic causal method.
- Largely used in computer science, to simulate random behaviors (truly random values can be too costly)
- Algorithmically generated -> can be used in a much simpler and frequent way.


# Random Number Generators

A seen, mediums to generate random sequences have been used for a long time in various
domains, from politics to cryptology.
However, they are not all equal / have bias which can be a problem depending on the usages.

## Definition

- RNG = device which can produce a sequence of random numbers, i.e. a without determinist properties and patterns.
- Can use physical interactions, computations, or a mix of both, to achieve it.

- Purely random sequences can't be described/generated by an algorithm (not recursive)
- Computational methods can only create pseudorandom seq -> PNRG

## Categories

### Generators based on physical phenomena

#### Traditionnal Methods

- Ancient methods (dice tossing or coin flipping,...) for random decisions or unpredictable seq.
- But often only random in appearances. coin flipping -> dynamics rules applied to the trajectory - - Seem random only because we can't simply measure the variables of the toss to solve the system. 
- But having even some meager hints about the initial conditions can still help deducing outcome
	- ex: the face on top before the coin is tossed may have more chances to be the resulting one

#### Quantic Phenomena
- Best phenomena to use as input for RNG = thus  possessing quantum mechanical physical randomness
	- Found  in quantum mechanics at the atomic or sub-atomic level.
	- ex of methods impl.:
		- measuring the nuclear decay with a Geiger counter
		- printing 0/1 when a photon is reflected/transmitted by a semi-transparent mirror
- Golden solutions, they are globally too costly to be democratized.

#### Noisy Phenomena
- Easier to detect and offer good results.
- Ex:
	- amplifying the thermal signal from a transistor or from an atmospheric radio receiver
	- comparison of pictures from an agitated scene
	- noise from analog-to-digital converter, etc
	
#### OS Solutions
- OS implement methods based on the unpredictable IO + behavior of the users

- Unix systems:
	/dev/urandom and /dev/random
	Device files probing analog sources (mouse, keyboard, disk acc, etc.)
	To harvest entropy and thus output random bytes; 

- Windows systems:
	CryptGenRandom
	Gather entropy though CPU counters, environment variables, threads IDs, etc.
	
- In both cases, entropy decrease during inactivity -> shortages

### Pseudo-Random Number Generators

- PRNG are generators based on algorithms.
- Seem paradoxical to associate randomness with algorithms, by definition deterministic
- But some clever implementation -> pseudo-random seq with periods long enough for their uses
- Because PRNG deterministic, output sequences totally defined by initial config -> state
- Key variable of state = seed (or random seed): number/vector which should be kept secret. 

#### LFSR

- LFSR = sequential shift register whose input bit is a linear function of its previous state
- Combinational logic that causes it to pseudo-randomly cycle through a sequence of bin val
- Mapping Fn2 -> Fn2 (vector space of all binary n-tuples)
- f = feedback function = Boolean operation of n variables
- Taps = bits of the register used in the linear operation (generally XOR)
- The feedback function can be expressed in finite field arithmetic as a poly mod 2
	- Feedback polynomial or reciprocal characteristic polynomial
- To obtain a maximal-length LFSR (period 2^n-1), conditions must be met by the feedback func:
	- Having an even number of taps
	- Using a relatively-prime set of taps
	
#### NLFSR

- Same theory as for LFSRs
- Onyl difference: the feedback function f is non-linear
- It makes NLFSRs harder to predict than LFSRs
- But also imposes extensive carefulness to ensure a maximal period of 2^n-1 bits.
- Lot of papers simply defining those conditions or listing the right config.

## Applications and Uses

- RNG have applications in every area where unpredictable behavior is desirable/required
- From cryptographic systems to gambling applications, statistical sampling, simulation, ...
- Various properties can be required from the generators:
	- Security application will need a cryptographically-secure RNG
	- Shuffling method will require uniqueness of the returned values
- Some require qualities mostly found in physical RNGs
- Others are satisfied with weaker forms of randomness (faster and lighter)
- Common to use a RNG to seed a strong PRNG, to increase the entropy

# Testing Randomness

Def of randomness is complex and app-dependent, so are the tests designed for RNGs.

## About the Difficulty to Test Randomness

- "Random sequence" -> various meanings depending on the field, so difficult to test
- Like for Crypto, large number of possibilities -> impossible to fully cover
	- ex: checking if a sequence has indeed no shorter construction is impossible without
checking every construction
- So randomness of a sequence is commonly analyzed through statistical tests or complexity evaluations (Kolmogorov).
- Battery of empirical statistical tests run against new RNG, to identify statistical bias.
- Does the RNG follow the hypothesis of perfect behavior:
	- values of the sequences "imitate independent random var from the uniform distribution"
- Different tests will thus detect different problems, various statistical behaviors
- Impossible to full cover -> no universal battery of tests
- So good RNGs are then those which pass complicated or numerous tests

## Common Tests

### DIEHARD Tests

- Developed by George Marsaglia, in 1995
- 15 tests, which are run over a large file containing the sequence, provided by the user


    Birthday spacings: Choose random points on a large interval. The spacings between the points should be asymptotically exponentially distributed.[clarification needed] The name is based on the birthday paradox.

    Overlapping permutations: Analyze sequences of five consecutive random numbers. The 120 possible orderings should occur with statistically equal probability.

    Ranks of matrices: Select some number of bits from some number of random numbers to form a matrix over {0,1}, then determine the rank of the matrix. Count the ranks.

    Monkey tests: Treat sequences of some number of bits as "words". Count the overlapping words in a stream. The number of "words" that don't appear should follow a known distribution. The name is based on the infinite monkey theorem.

    Count the 1s: Count the 1 bits in each of either successive or chosen bytes. Convert the counts to "letters", and count the occurrences of five-letter "words".

    Parking lot test: Randomly place unit circles in a 100 x 100 square. If the circle overlaps an existing one, try again. After 12,000 tries, the number of successfully "parked" circles should follow a certain normal distribution.

    Minimum distance test: Randomly place 8,000 points in a 10,000 x 10,000 square, then find the minimum distance between the pairs. The square of this distance should be exponentially distributed with a certain mean.

    Random spheres test: Randomly choose 4,000 points in a cube of edge 1,000. Center a sphere on each point, whose radius is the minimum distance to another point. The smallest sphere's volume should be exponentially distributed with a certain mean.

    The squeeze test: Multiply 231 by random floats on (0,1) until you reach 1. Repeat this 100,000 times. The number of floats needed to reach 1 should follow a certain distribution.

    Overlapping sums test: Generate a long sequence of random floats on (0,1). Add sequences of 100 consecutive floats. The sums should be normally distributed with characteristic mean and sigma.

    Runs test: Generate a long sequence of random floats on (0,1). Count ascending and descending runs. The counts should follow a certain distribution.

    The craps test: Play 200,000 games of craps, counting the wins and the number of throws per game. Each count should follow a certain distribution.


### TestU01 Suite

- Software library, initiated in 1985, in ANSI C language
- Collection of utilities
- Implementations of classical statistical tests + others from literature + original ones
- Also offers toos to implement specific statsitical tests.

### Berlekamp-Massey Algorithm

#### Definition
- Algo, not really a test. Used to find in a field Fk the min poly of linearly recurrent sequences, - Invented by Elwyn Berlekamp in 1967 for decoding BCH codes
- Adapted + simplified by James Massey
- Determine minimal degree L + annihilator (or inverse feedback) polynomial F(x) of the given sequence S

#### Algorithm
- Finding this minimal polynomial requires to solve a set of L linear equations
- F(x) being indeed uniquely determined by the 1st 2L elements of S
- At each iteration, it evaluates the discrepancy (difference between things that should be the same)
- If beta = 0, F(x) and L are still currently correct -> go next iteration
- If beta != 0, F(x) should be concordantly adjusted, by shifting + scaling the syndromes added since the last update of L 
- In order to keep track of the number of errors and current degree of the
polynomial, L should be updated, otherwise the discrepancies will reach 0 before l grows bigger than 2L.

#### Implementation

# Conclusion